# Seattle Collision Prediction
### **Introduction:**
####  *Seattle is the largest city in both the state of Washington and the Pacific Northwest region of North America. According to U.S. Census data released in 2019, the Seattle metropolitan area's population stands at 3.98 million, making it the 15th-largest in the United States. In July 2013, Seattle was the fastest-growing major city in the United States and remained in the top five in May 2015 with an annual growth rate of 2.1%. In July 2016, Seattle was again the fastest-growing major U.S. city, with a 3.1% annual growth rate. it’s home to a large tech industry, with Microsoft and Amazon headquartered in its metropolitan area.*
### **Business Problem:**
#### *Being one of the most populous and a fast paced city of America with tech powerhouses in it , This city is under surge for further improvement. The city is crowded with a huge count of skyscrappers,so they look too dense in structure The Open Data Program makes the data generated by the City of Seattle has been openly available to the public for the purpose of increasing the quality of life for the residents, increasing transparency, accountability and comparability, promoting economic development and research, and improving internal performance management.*

#### *The Traffic Records Group, Traffic Management Division, Seattle Department of Transportation, provides data for all collisions and crashes that have occured in the state from 2004 to the present day. The data is updated weekly and can be found at the Seattle Open GeoData Portal.*

#### *The objective is to exploit this data to extract vital features that would enable us to end up with a good model that would enable the prediction of the severity of future accidents that take place in the state. This would further enable the Department of Transportation to prioritise their SOPs and channel their energy to ensure that fewer fatalities result in automobile collisions.*

### **Data Acquisition:**
#### *The dataset is available as comma-separated values (CSV) files, KML files, and ESRI shapefiles that can be downloaded from the Seattle Open GeoData Portal. The data is also available from RESTful API services in formats such as GeoJSON.eventhough the dataset is publicly available in seattle dataset portal, the data was inaccessible during the time when i started the project ,there wa a internal server issue.fortunately ,I got the same dataset which was posted on kaggle.*
#### *The metadata of the dataset can be found from the website of the Seattle Department of Transportation. On reading the dataset summary, we can determine the description of each of the fields and their possible values.*

### **Data Understanding:**
#### *The Data can be easily understood by seeing the metadata pdf file commited to this github repository*
#### *The dataset contains 221k+ records and 40 fields.The data contains several categorical fields and corresponding descriptions which could help us in further analysis. We make an attempt at understanding the data in terms of the fields that we shall take into account for later stages of model building.*

### **Data Cleaning & Pre-Processing:**
#### *The dataset is huge and it consist of many irrelevant fields for this dataset and many missing values in each column as it is sourced from a database table which has many unique features which are not suitable for further analysis .*
#### *The SPEEDING field classifies collisions based on whether or not speeding was a factor in the collision. Blanks indicate cases where the vehicle was not speeding.* 
#### *The UNDERINFL field describes whether or not a driver involved was under the influence of drugs or alcohol. The values 0 and N denote that the driver was not under any influence while 1 and Y that they were.*
#### *OBJECTID, INCKEY, COLDETKEY, INTKEY, SEGLANEKEY, CROSSWALKKEY, REPORTNO, EXCEPTRSNCODE, SDOT_COLCODE, SDOTCOLNUM and LOCATION are not useful for further processing.so we are dropping out those columns.*
#### *WEATHER, ROADCOND, LIGHT_COND and some other columns are containing categorical variables such as Other and Unknown which are some unwanted categories for prediction so converting them to NaN .so that we can drop those data too along with the missing data.*
#### *We now do an one-hot encoding of the WEATHER, ROADCOND, and LIGHTCOND fields as they are categorical.*
#### *The datasets x and y are constructed. The set x contains all the training examples and y contains all the labels. Feature scaling of data is done to normalize the data in a dataset to a specific range.*
#### *After normalization, they are split into x_train, y_train, x_test, and y_test. The first two sets sahll be used for training and the last two shall be used for testing. Upon choosing a suitable split ratio, 80% of data is used for training and 20% of is used for testing.*

### **Modelling and Evaluation**

#### *Decision Tree makes decision with tree-like model. It splits the sample into two or more homogenous sets (leaves) based on the most significant differentiators in the input variables. To choose a differentiator (predictor), the algorithm considers all features and does a binary split on them (for categorical data, split by category; for continuous, pick a cut-off threshold). It will then choose the one with the least cost (i.e. highest accuracy), and repeats recursively, until it successfully splits the data in all leaves (or reaches the maximum depth).*
#### *Information gain for a decision tree classifier can be calculated either using the Gini Index measure or the Entropy measure, whichever gives a greater gain. A hyper parameter Decision Tree Classifier was used to decide which tree to use, DTC using entropy had greater information gain; hence it was used for this classification problem.*
#### *Random Forest Classifier is an ensemble (algorithms which combines more than one algorithms of same or different kind for classifying objects) tree-based learning algorithm. RFC is a set of decision trees from randomly selected subset of training set. It aggregates the votes from different decision trees to decide the final class of the test object. Used for both classification and regression.*
#### *Similar to DTC, RFT requires an input that specifies a measure that is to be used for classification, along with that a value for the number of estimators (number of decision trees) is required. A hyper parameter RFT was used to determine the best choices for the above mentioned parameters. RFT with 75 DT’s using entropy as the measure gave the best accuracy when trained and tested on pre-processed accident severity dataset.*
#### *Logistic Regression is a classifier that estimates discrete values (binary values like 0/1, yes/no, true/false) based on a given set of an independent variables. It basically predicts the probability of occurrence of an event by fitting data to a logistic function. Hence it is also known as logistic regression. The values obtained would always lie within 0 and 1 since it predicts the probability.*
#### *The chosen dataset has more than two target categories in terms of the accident severity code assigned, one-vs-one (OvO) strategy is employed.*
### **Results**
#### *The accuracies of all models was 100% which means we can accurately predict the severity of an accident. A bar plot is plotted below with the bars representing the accuracy of each model.*
### **Conclusion**
#### *Initially, the classifiers had an prediction accuracy of 66%-71%, however, upon going back to the data preparation phase, minor tweaking and taking additional fields in the dataset improved the overall accuracy of all models.*
#### *The accuracy of the classifiers is excellent, i.e. 100%. This means that the model has trained well and fits the training data and performs well on the testing set as well as the training set. We can conclude that this model can accurately predict the severity of car accidents in Seattle.*
### **Future Work**
#### *The trained model can be deployed onto governance and monitoring web and mobile applications to predict the accident severity for a given set of parameters.*

